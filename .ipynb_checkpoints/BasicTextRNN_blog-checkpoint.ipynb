{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pp\n",
    "import seaborn as sb\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "pp.style.use(\"fivethirtyeight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"/Users/jeanbaptiste/Downloads/train.tsv\", sep = \"\\t\")\n",
    "train_df = train_df.sample(frac = 1.0)\n",
    "print(train_df.columns)\n",
    "print(train_df.shape[0])\n",
    "train_df.loc[pd.isnull(train_df[\"item_description\"]), \"item_description\"] = \"\"\n",
    "train_df[\"total_text\"] = train_df[\"item_description\"] + \" \" + train_df[\"name\"]\n",
    "train_df.drop([\"item_description\", \"name\"], axis = 1, inplace = True)\n",
    "pp.show(sb.kdeplot(np.log(1 + train_df[\"price\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\.\", \" \\. \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "train_df[\"total_text\"] = train_df[\"total_text\"].apply(clean_str)\n",
    "print(train_df[\"total_text\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloveModel :\n",
    "    def __init__(self, path) :\n",
    "        i = 1\n",
    "        self.index_word = []\n",
    "        self.embeddings = []\n",
    "        self.word_index = {}\n",
    "        with open(path, \"r\") as file :\n",
    "            row = file.readline().split(\" \")\n",
    "            while row != [\"\"] :\n",
    "                self.index_word.append(row[0])\n",
    "                self.embeddings.append(np.array(row[1:], dtype = np.float32))\n",
    "                self.word_index[row[0]] = i\n",
    "                i += 1\n",
    "                row = file.readline().split(\" \")\n",
    "        self.index_words = [\"\"] + self.index_word\n",
    "        self.embeddings = [np.zeros_like(self.embeddings[0])] + self.embeddings\n",
    "\n",
    "stanford_glove = GloveModel(\"/Users/jeanbaptiste/Downloads/glove/glove.6B.300d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sequence(string, dict_words) :\n",
    "    splitted = string.split(\" \")\n",
    "    result = []\n",
    "    for word in splitted :\n",
    "        try :\n",
    "            result.append(dict_words[word])\n",
    "        except KeyError :\n",
    "            pass\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = list(train_df[\"total_text\"].apply(text_to_sequence, args = (stanford_glove.word_index,)))\n",
    "sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen = 150)\n",
    "sequences = np.array(sequences).astype(np.int32)\n",
    "print(sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.log(1 + train_df[\"price\"].values)\n",
    "y_train = target[:1000000]\n",
    "y_test = target[1000000:]\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(y_train.reshape((-1, 1)))\n",
    "y_train_scaled = scaler.transform(y_train.reshape((-1, 1))).flatten().clip(-2.5, 2.5)\n",
    "y_test_scaled = scaler.transform(y_test.reshape((-1, 1))).flatten()\n",
    "X_train = sequences[:1000000, :]\n",
    "X_test = sequences[1000000:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_regularized_lstm_cell(nb_hidden, keep_prob) :\n",
    "    return tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.BasicLSTMCell(nb_hidden),\n",
    "                                         output_keep_prob = keep_prob,\n",
    "                                         state_keep_prob = 1.0\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicTextRNN :\n",
    "    def __init__(self) :\n",
    "        pass\n",
    "        \n",
    "    def create_model(self, embedding_model, nb_hidden_lstm, keep_prob = 0.8, nb_layers = 3) :\n",
    "        \n",
    "        self.input_x = tf.placeholder(dtype = tf.int32, shape = [None, 150], name = \"input_x\")\n",
    "        self.input_y = tf.placeholder(dtype = tf.float32, shape = [None], name = \"input_y\")\n",
    "\n",
    "        with tf.device(\"/gpu:0\") :\n",
    "            \n",
    "            #Create embedding with pretrained model weights\n",
    "            self.embedding = tf.Variable(np.vstack(embedding_model.embeddings),\n",
    "                                         dtype = tf.float32,\n",
    "                                         trainable = False,\n",
    "                                         name = \"sf_embedding\"\n",
    "                                        )  \n",
    "            \n",
    "            self.embedding_lookup = tf.nn.embedding_lookup(self.embedding,\n",
    "                                                           self.input_x,\n",
    "                                                           name = \"embedding_lookup\"\n",
    "                                                          )\n",
    "            \n",
    "            #Unstacking words vector representations to feed LSTM layer\n",
    "            self.unstacked_embedding = tf.unstack(self.embedding_lookup,\n",
    "                                                  num = 150,\n",
    "                                                  axis = 1,\n",
    "                                                  name = \"unstacked_embeddings\"\n",
    "                                                 )\n",
    "            #defining multi-layer RNN\n",
    "            \n",
    "            #defining cells\n",
    "            self.lstm_cells = [create_regularized_lstm_cell(nb_hidden_lstm, keep_prob) for _ in range(nb_layers)]\n",
    "            \n",
    "            #stacking cells\n",
    "            self.multi_lstm_layer = tf.contrib.rnn.MultiRNNCell(self.lstm_cells)\n",
    "            \n",
    "            #connecting cells with input\n",
    "            self.lstm_outputs, self.lstm_states = tf.contrib.rnn.static_rnn(self.multi_lstm_layer,\n",
    "                                                                            self.unstacked_embedding,\n",
    "                                                                            dtype = tf.float32\n",
    "                                                                           )\n",
    "            #defining weights and bias for output\n",
    "            self.W = tf.Variable(tf.random_normal(stddev = 0.1, shape = (nb_hidden_lstm, 1)),\n",
    "                                 name = \"W\",\n",
    "                                 dtype = tf.float32\n",
    "                                )\n",
    "            self.b = tf.Variable(tf.random_normal(stddev = 0.1, shape = [1]),\n",
    "                                 name = \"b\",\n",
    "                                 dtype = tf.float32\n",
    "                                )\n",
    "\n",
    "            self.output = tf.matmul(self.lstm_outputs[-1], self.W) + self.b\n",
    "            #defining loss\n",
    "            self.loss = tf.reduce_mean(tf.square(self.input_y - self.output), name = \"loss\")\n",
    "    \n",
    "    def set_optimizer(self, learning_rate = 0.0001, decay_rate = 0.9999) :\n",
    "        \n",
    "        self.global_step = tf.Variable(0, trainable = False)\n",
    "        \n",
    "        evolutive_lr = tf.train.exponential_decay(learning_rate = learning_rate,\n",
    "                                                  global_step = self.global_step,\n",
    "                                                  decay_rate = decay_rate,\n",
    "                                                  decay_steps = 1\n",
    "                                                 )\n",
    "        \n",
    "        self.optimizer = tf.train.RMSPropOptimizer(evolutive_lr, name = \"optimizer\")\n",
    "        \n",
    "        self.train_op = self.optimizer.minimize(self.loss, global_step = self.global_step)\n",
    "                        \n",
    "    def fit_with_early_stopping(self, x, y, x_test, y_test, nb_epoch, early_stopping = 20, batch_size = 250,\n",
    "                                step = 50000, init = True) :\n",
    "        \n",
    "        nb_obs_train = len(x)\n",
    "        nb_obs_test = len(x_test)\n",
    "        self.test_errors_log = []\n",
    "        early_stopping_count = 0\n",
    "        self.train_error_log = []\n",
    "        saver = tf.train.Saver()\n",
    "        #we use _step because is step is not a multiple of batch size (i % step == 0) will always be false\n",
    "        _step = step % batch_size\n",
    "        \n",
    "        with tf.Session() as sess :\n",
    "            if init :\n",
    "                _init = tf.global_variables_initializer()\n",
    "                sess.run(_init)\n",
    "            for epoch in range(nb_epoch) :\n",
    "                batch_errors = []\n",
    "                avg_cost = 0.0\n",
    "                i = 0\n",
    "                test_loss_improvement = 0\n",
    "                # fitting on train sample\n",
    "                while (i + batch_size <= nb_obs_train - 1) :\n",
    "                    _, c = sess.run([self.train_op, self.loss], \n",
    "                                    feed_dict = {self.input_x : x[i : i + batch_size, :],\n",
    "                                                 self.input_y : y[i : i + batch_size]})\n",
    "                    #saving training loss\n",
    "                    batch_errors.append(c)\n",
    "                    i += batch_size\n",
    "                    if (i % step == _step) :\n",
    "                        j = 0\n",
    "                        test_errors = []\n",
    "                        #testing performance on test sample\n",
    "                        while (j + batch_size <= nb_obs_test - 1) :\n",
    "                            test_error = sess.run([self.loss],\n",
    "                                                  feed_dict = {self.input_x : x_test[j : j + batch_size, :],\n",
    "                                                               self.input_y : y_test[j : j + batch_size]})\n",
    "                            test_errors.append(test_error)\n",
    "                            j += batch_size\n",
    "                        test_error_mean = np.mean(test_errors)\n",
    "                        self.test_errors_log.append(test_error_mean)\n",
    "                        batch_errors_mean = np.mean(batch_errors)\n",
    "                        self.train_error_log.append(batch_errors_mean)\n",
    "                        #reset batch errors log\n",
    "                        batch_errors = []\n",
    "                        print(\"train error : \" + str(batch_errors_mean) + \n",
    "                                         \" test error : \" + str(test_error_mean))\n",
    "                        #saving model if loss is the best on test set\n",
    "                        if test_error_mean == min(self.test_errors_log) :\n",
    "                            saver.save(sess, \"/Users/jeanbaptiste/TextRNN\")\n",
    "                            early_stopping_count = 0\n",
    "                        else : \n",
    "                            early_stopping_count += 1\n",
    "                    #stop if early stopping limit has been reached\n",
    "                    if early_stopping_count == early_stopping :\n",
    "                        print(\"early stopping reached, best test value : \" + str(min(self.test_errors_log)))\n",
    "                        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_text_cnn = BasicTextRNN() \n",
    "first_text_cnn.create_model(stanford_glove, 300, nb_layers = 6, keep_prob = 0.8)\n",
    "first_text_cnn.set_optimizer()\n",
    "first_text_cnn.fit_with_early_stopping(X_train, y_train, X_test, y_test, 50, batch_size = 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
