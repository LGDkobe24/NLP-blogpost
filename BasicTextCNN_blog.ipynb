{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pp\n",
    "import seaborn as sb\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "pp.style.use(\"fivethirtyeight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"/Users/jeanbaptiste/Downloads/train.tsv\", sep = \"\\t\")\n",
    "train_df = train_df.sample(frac = 1.0)\n",
    "print(train_df.columns)\n",
    "print(train_df.shape[0])\n",
    "train_df.loc[pd.isnull(train_df[\"item_description\"]), \"item_description\"] = \"\"\n",
    "train_df[\"total_text\"] = train_df[\"item_description\"] + \" \" + train_df[\"name\"]\n",
    "train_df.drop([\"item_description\", \"name\"], axis = 1, inplace = True)\n",
    "pp.show(sb.kdeplot(np.log(1 + train_df[\"price\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\.\", \" \\. \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "train_df[\"total_text\"] = train_df[\"total_text\"].apply(clean_str)\n",
    "print(train_df[\"total_text\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloveModel :\n",
    "    def __init__(self, path) :\n",
    "        i = 1\n",
    "        self.index_word = []\n",
    "        self.embeddings = []\n",
    "        self.word_index = {}\n",
    "        with open(path, \"r\") as file :\n",
    "            row = file.readline().split(\" \")\n",
    "            while row != [\"\"] :\n",
    "                self.index_word.append(row[0])\n",
    "                self.embeddings.append(np.array(row[1:], dtype = np.float32))\n",
    "                self.word_index[row[0]] = i\n",
    "                i += 1\n",
    "                row = file.readline().split(\" \")\n",
    "        self.index_words = [\"\"] + self.index_word\n",
    "        self.embeddings = [np.zeros_like(self.embeddings[0])] + self.embeddings\n",
    "\n",
    "stanford_glove = GloveModel(\"/Users/jeanbaptiste/Downloads/glove/glove.6B.300d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sequence(string, dict_words) :\n",
    "    splitted = string.split(\" \")\n",
    "    result = []\n",
    "    for word in splitted :\n",
    "        try :\n",
    "            result.append(dict_words[word])\n",
    "        except KeyError :\n",
    "            pass\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = list(train_df[\"total_text\"].apply(text_to_sequence, args = (stanford_glove.word_index,)))\n",
    "sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen = 150)\n",
    "sequences = np.array(sequences).astype(np.int32)\n",
    "print(sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.log(1 + train_df[\"price\"].values)\n",
    "y_train = target[:1000000]\n",
    "y_test = target[1000000:]\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(y_train.reshape((-1, 1)))\n",
    "y_train_scaled = scaler.transform(y_train.reshape((-1, 1))).flatten().clip(-2.5, 2.5)\n",
    "y_test_scaled = scaler.transform(y_test.reshape((-1, 1))).flatten()\n",
    "X_train = sequences[:1000000, :]\n",
    "X_test = sequences[1000000:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicTextCNN :\n",
    "    def __init__(self) :\n",
    "        pass\n",
    "        \n",
    "    def create_conv_model(self, embedding_model, filter_shape, nb_filters, keep_prob) :\n",
    "        \n",
    "        self.input_x = tf.placeholder(dtype = tf.int32, shape = [None, 150], name = \"input_x\")\n",
    "        self.input_y = tf.placeholder(dtype = tf.float32, shape = [None], name = \"input_y\")\n",
    "        \n",
    "        with tf.device(\"/gpu:0\") :\n",
    "            #Create embedding with pretrained model weights\n",
    "            self.embedding = tf.Variable(np.vstack(embedding_model.embeddings),\n",
    "                                         dtype = tf.float32,\n",
    "                                         trainable = False,\n",
    "                                         name = \"sf_embedding\"\n",
    "                                         )            \n",
    "            self.embedding_lookup = tf.nn.embedding_lookup(self.embedding, self.input_x, name = \"embedding_lookup\")\n",
    "            #Embedding expansion for compatibility with conv layer\n",
    "            self.expanded_embedding = tf.expand_dims(self.embedding_lookup, -1, name = \"expanded_embedding\")\n",
    "            \n",
    "            #convolution layer\n",
    "            self.conv1 = tf.layers.conv2d(inputs = self.expanded_embedding,\n",
    "                                          padding = \"same\", \n",
    "                                          filters = nb_filters,\n",
    "                                          kernel_size = filter_shape,\n",
    "                                          name = \"conv_layer1\",\n",
    "                                          activation = tf.nn.leaky_relu\n",
    "                                          )\n",
    "            #first max pooling layer\n",
    "            self.max_pool1 = tf.layers.max_pooling2d(self.conv1,\n",
    "                                                     pool_size = [3, 3],\n",
    "                                                     strides = [3, 3],\n",
    "                                                     name = \"max_pool_layer1\"\n",
    "                                                    )\n",
    "            #second convolution layer\n",
    "            self.conv2 = tf.layers.conv2d(inputs = self.max_pool1,\n",
    "                                          padding = \"same\", \n",
    "                                          filters = nb_filters,\n",
    "                                          kernel_size = filter_shape,\n",
    "                                          name = \"conv_layer2\",\n",
    "                                          activation = tf.nn.leaky_relu\n",
    "                                         )\n",
    "            #second max pooling layer\n",
    "            self.max_pool2 = tf.layers.max_pooling2d(self.conv2,\n",
    "                                                     pool_size = [1, 2],\n",
    "                                                     strides = [1, 2],\n",
    "                                                     name = \"max_pool_layer2\"\n",
    "                                                     )\n",
    "            #third conv layer\n",
    "            self.conv3 = tf.layers.conv2d(inputs = self.max_pool2,\n",
    "                                          padding = \"same\", \n",
    "                                          filters = nb_filters * 2,\n",
    "                                          kernel_size = filter_shape,\n",
    "                                          name = \"conv_layer3\",\n",
    "                                          activation = tf.nn.leaky_relu\n",
    "                                         )\n",
    "            #pool flatting to enable dense connexion\n",
    "            self.flat_pool = tf.reshape(self.conv3, [-1, 50*50*nb_filters*2])\n",
    "            #first relu activation with dropout\n",
    "            self.W1 = tf.Variable(tf.random_normal(stddev = 0.1, shape = [50*50*nb_filters*2, 2500]),\n",
    "                                                   dtype = tf.float32,\n",
    "                                                   name = \"W1\"\n",
    "                                                   )\n",
    "            self.b1 = tf.Variable(tf.random_normal(stddev = 0.1, shape = [2500]), dtype = tf.float32, name = \"b1\")\n",
    "            self.relu_activation1 = tf.nn.leaky_relu(tf.matmul(self.flat_pool, self.W1) + self.b1)\n",
    "            self.relu_activation1_regularized = tf.nn.dropout(self.relu_activation1,\n",
    "                                                              keep_prob = keep_prob,\n",
    "                                                              name = \"relu1_regularized\"\n",
    "                                                             )\n",
    "            #second relu activation with dropout\n",
    "            self.W2 = tf.Variable(tf.random_normal(stddev = 0.1,\n",
    "                                                   shape = [2500, 500]),\n",
    "                                                   dtype = tf.float32,\n",
    "                                                   name = \"W2\"\n",
    "                                                   )\n",
    "            self.b2 = tf.Variable(tf.random_normal(stddev = 0.1, shape = [500]),\n",
    "                                                   dtype = tf.float32,\n",
    "                                                   name = \"b2\")\n",
    "            self.relu_activation2 = tf.nn.leaky_relu(tf.matmul(self.relu_activation1_regularized, self.W2) + self.b2,\n",
    "                                                     name = \"relu_activation2\")\n",
    "            self.relu_activation2_regularized = tf.nn.dropout(self.relu_activation2,\n",
    "                                                              keep_prob = keep_prob, name = \"relu2_regularized\")\n",
    "            #output vector definition\n",
    "            self.W3 = tf.Variable(tf.random_normal(stddev = 0.1, shape = [500, 1]), name = \"output_weights\")\n",
    "            self.b3 = tf.Variable(tf.random_normal(stddev = 0.1 , shape = [1]), name = \"output_biases\")\n",
    "            self.output = tf.matmul(self.relu_activation2_regularized, self.W3) + self.b3\n",
    "            #loss definition\n",
    "            self.loss = tf.reduce_mean(tf.square(self.input_y - self.output), name = \"loss\")\n",
    "    \n",
    "    def set_optimizer(self, learning_rate = 0.0001, decay_rate = 0.99999) :\n",
    "        \n",
    "        self.global_step = tf.Variable(0, trainable = False)\n",
    "        \n",
    "        evolutive_lr = tf.train.exponential_decay(learning_rate = learning_rate,\n",
    "                                                  global_step = self.global_step,\n",
    "                                                  decay_rate = decay_rate,\n",
    "                                                  decay_steps = 1\n",
    "                                                 )\n",
    "        \n",
    "        self.optimizer = tf.train.AdamOptimizer(evolutive_lr,\n",
    "                                                epsilon = 1.0,\n",
    "                                                name = \"optimizer\"\n",
    "                                               )\n",
    "        \n",
    "        self.train_op = self.optimizer.minimize(self.loss, global_step = self.global_step)\n",
    "                        \n",
    "    def fit_with_early_stopping(self, x, y, x_test, y_test, nb_epoch, early_stopping = 10, batch_size = 20,\n",
    "                                step = 50000, init = True) :\n",
    "        \n",
    "        nb_obs_train = len(x)\n",
    "        nb_obs_test = len(x_test)\n",
    "        self.test_errors_log = []\n",
    "        early_stopping_count = 0\n",
    "        self.train_error_log = []\n",
    "        saver = tf.train.Saver()\n",
    "        #we use step because is step is not a multiple of batch size (i % step == 0) will always be false\n",
    "        _step = step % batch_size\n",
    "        \n",
    "        with tf.Session() as sess :\n",
    "            if init :\n",
    "                _init = tf.global_variables_initializer()\n",
    "                sess.run(_init)\n",
    "            for epoch in range(nb_epoch) :\n",
    "                batch_errors = []\n",
    "                avg_cost = 0.0\n",
    "                i = 0\n",
    "                test_loss_improvement = 0\n",
    "                # fitting on train sample\n",
    "                while (i + batch_size <= nb_obs_train - 1) :\n",
    "                    _, c = sess.run([self.train_op, self.loss], \n",
    "                                    feed_dict = {self.input_x : x[i : i + batch_size, :],\n",
    "                                                 self.input_y : y[i : i + batch_size]})\n",
    "                    #saving training loss\n",
    "                    batch_errors.append(c)\n",
    "                    i += batch_size\n",
    "                    if (i % step == _step) :\n",
    "                        j = 0\n",
    "                        test_errors = []\n",
    "                        #testing performance on test sample\n",
    "                        while (j + batch_size <= nb_obs_test - 1) :\n",
    "                            test_error = sess.run([self.loss],\n",
    "                                                  feed_dict = {self.input_x : x_test[j : j + batch_size, :],\n",
    "                                                               self.input_y : y_test[j : j + batch_size]})\n",
    "                            test_errors.append(test_error)\n",
    "                            j += batch_size\n",
    "                        test_error_mean = np.mean(test_errors)\n",
    "                        self.test_errors_log.append(test_error_mean)\n",
    "                        batch_errors_mean = np.mean(batch_errors)\n",
    "                        self.train_error_log.append(batch_errors_mean)\n",
    "                        #reset batch errors log\n",
    "                        batch_errors = []\n",
    "                        print(\"train error : \" + str(batch_errors_mean) + \n",
    "                                         \" test error : \" + str(test_error_mean))\n",
    "                        #saving model if loss is the best on test set\n",
    "                        if test_error_mean == min(self.test_errors_log) :\n",
    "                            saver.save(sess, \"/Users/jeanbaptiste/TextCNN\")\n",
    "                            early_stopping_count = 0\n",
    "                        else : \n",
    "                            early_stopping_count += 1\n",
    "                    #stop if early stopping limit has been reached\n",
    "                    if early_stopping_count == early_stopping :\n",
    "                        print(\"early stopping reached, best test value : \" + str(min(self.test_errors_log)))\n",
    "                        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_text_cnn = BasicTextCNN() \n",
    "first_text_cnn.create_conv_model(stanford_glove, filter_shape = (3, 3), nb_filters = 3, keep_prob = 0.8)\n",
    "first_text_cnn.set_optimizer()\n",
    "first_text_cnn.fit_with_early_stopping(X_train, y_train, X_test, y_test, 250, batch_size = 5)"
    "first_text_cnn.fit_with_early_stopping(X_train, y_train, X_test, y_test, 25, batch_size = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
